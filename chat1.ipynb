{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "# load and set our key\n",
    "openai.api_key = open(\"key.txt\", \"r\").read().strip(\"\\n\")\n",
    "\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User's input was:  I'm using multiple functions to calculate new columns of a dataframe, how can I use multiprocessing to execute the functions simutaneously?\n",
      "To use multiprocessing in Python to execute multiple functions simultaneously, which each apply some calculation and create a new column in a DataFrame, you need to follow these steps:\n",
      "\n",
      "1. Import the necessary libraries.\n",
      "2. Define functions that will perform calculations and add new columns to the input DataFrames.\n",
      "3. Set up a multiprocessing pool to execute these functions in parallel.\n",
      "4. Collect the results and assemble the final DataFrame.\n",
      "\n",
      "Below is an example that demonstrates this process using the `multiprocessing` library:\n",
      "\n",
      "```python\n",
      "import pandas as pd\n",
      "import multiprocessing\n",
      "\n",
      "# Sample DataFrame\n",
      "data = {\n",
      "    'A': [1, 2, 3, 4],\n",
      "    'B': [5, 6, 7, 8],\n",
      "}\n",
      "df = pd.DataFrame(data)\n",
      "\n",
      "# Define functions to be applied to the DataFrame\n",
      "def add_column1(df):\n",
      "    df['C'] = df['A'] + df['B']\n",
      "    return df\n",
      "\n",
      "def add_column2(df):\n",
      "    df['D'] = df['A'] * df['B']\n",
      "    return df\n",
      "\n",
      "def add_column3(df):\n",
      "    df['E'] = df['A'] - df['B']\n",
      "    return df\n",
      "\n",
      "# List of functions to be executed in parallel\n",
      "functions_to_apply = [add_column1, add_column2, add_column3]\n",
      "\n",
      "# Function to apply the functions in parallel using multiprocessing\n",
      "def apply_functions_in_parallel(df, functions):\n",
      "    with multiprocessing.Pool(processes=len(functions)) as pool:\n",
      "        # Apply each function to the dataframe in parallel.\n",
      "        # The result will be a list of DataFrames with new columns.\n",
      "        results = pool.map(lambda f: f(df.copy()), functions)\n",
      "    return results\n",
      "\n",
      "# Run the functions in parallel\n",
      "result_dataframes = apply_functions_in_parallel(df, functions_to_apply)\n",
      "\n",
      "# Combine the results back into a single DataFrame\n",
      "for result_df in result_dataframes:\n",
      "    df = pd.concat([df, result_df.drop(df.columns, axis=1)], axis=1)\n",
      "\n",
      "print(df)\n",
      "```\n",
      "\n",
      "Notes:\n",
      "- `apply_functions_in_parallel` uses a multiprocessing pool to apply a list of functions onto the DataFrame. Each function is given a copy of the DataFrame to avoid race conditions where processes might try to write to the DataFrame at the same time.\n",
      "- The `lambda` function is used to map the function to apply with the DataFrame copy as its argument.\n",
      "- The resulting DataFrames are combined by concatenating the new columns back into the original DataFrame.\n",
      "- You should be cautious with the amount of data and memory use when copying data across processes like this. If the DataFrame is very large, this method might not be efficient or even feasible due to the memory overhead.\n",
      "\n",
      "Keep in mind that due to the Global Interpreter Lock (GIL) in CPython, multiprocessing can be beneficial for CPU-bound tasks, but it might not provide speedup for I/O-bound tasks or tasks that already run fast enough on a single thread. Furthermore, the overhead of setting up and communicating between processes might negate the benefits of parallel execution for small DataFrames. It's always good to profile your code to determine if parallelization is the right choice for your specific case.\n",
      "\n",
      "User's input was:  How do I pass args to the functions?\n",
      "If you need to pass additional arguments to the functions that are used to calculate new columns in your DataFrame, you can do so by using a wrapper function or by using the `partial` function from the `functools` library.\n",
      "\n",
      "Here's an example using the `functools.partial` to pass arguments to functions that are called within a multiprocessing pool:\n",
      "\n",
      "```python\n",
      "import pandas as pd\n",
      "from multiprocessing import Pool\n",
      "from functools import partial\n",
      "\n",
      "# Your DataFrame\n",
      "data = {'A': [1, 2, 3, 4], 'B': [10, 20, 30, 40]}\n",
      "df = pd.DataFrame(data)\n",
      "\n",
      "# Define functions that take the dataframe and additional arguments\n",
      "def add_column1(df, factor):\n",
      "    df['C'] = (df['A'] + df['B']) * factor\n",
      "    return df\n",
      "\n",
      "def add_column2(df, addend):\n",
      "    df['D'] = df['A'] * df['B'] + addend\n",
      "    return df\n",
      "\n",
      "# Function to run a given function f with additional arguments\n",
      "def apply_func(data, func):\n",
      "    df, args, kwargs = data\n",
      "    return func(df, *args, **kwargs)\n",
      "\n",
      "# Set up a list of functions with their corresponding arguments\n",
      "functions_args = [\n",
      "    (add_column1, (2,)),       # add_column1 will be called with the factor argument set to 2\n",
      "    (add_column2, (5,)),       # add_column2 will be called with the addend argument set to 5\n",
      "]\n",
      "\n",
      "# Prepare the data to be passed to the apply function\n",
      "data_for_apply = [(df.copy(), args, {}) for func, args in functions_args]\n",
      "\n",
      "# Set up a multiprocessing pool and execute the functions in parallel\n",
      "with Pool() as pool:\n",
      "    # Use `apply_func` as the worker function to match the `Pool.map` interface, args for each function call\n",
      "    results = pool.map(apply_func, [(df.copy(), args, {}) for func, args in functions_args])\n",
      "\n",
      "# Combine all the resulting DataFrames\n",
      "final_df = df.copy()\n",
      "for result in results:\n",
      "    # `result` is a DataFrame with one or more new columns; merge them into final_df\n",
      "    for col in result.columns.difference(final_df.columns):\n",
      "        final_df[col] = result[col]\n",
      "\n",
      "print(final_df)\n",
      "```\n",
      "\n",
      "In this example:\n",
      "\n",
      "- Each function along with its respective additional arguments are stored in a tuple in the `functions_args` list.\n",
      "- The `apply_func` function is a generic function that unpacks these tuples and calls the respective calculation function with the corresponding arguments. This is necessary because `Pool.map` can only pass a single iterable to the worker function.\n",
      "- The `pool.map` method is called with the `apply_func` as the target function to apply each function from the list to the DataFrame.\n",
      "\n",
      "Remember that when you use multiprocessing, workers cannot directly share the state of the main process, so each worker will receive a copy of the DataFrame, which could potentially be memory-intensive if the DataFrame is large. Be sure to consider this aspect when designing your multiprocessing code for DataFrame manipulation.\n",
      "\n",
      "User's input was:  TypeError: apply_func() missing 1 required positional argument: 'func'\n",
      "I apologize for the confusion. The `apply_func` example I provided was not correctly formulated to work with `Pool.map()` due to providing the data to be passed to `apply_func` in the wrong manner. Instead, you should pass a tuple where the first element is the function and the rest are the function's arguments. Moreover, `apply_func` does not need to unpack the DataFrame because the DataFrame is a common argument across all functions.\n",
      "\n",
      "Let's correct this using the `pool.starmap()` method, which allows us to pass multiple arguments to the worker function:\n",
      "\n",
      "```python\n",
      "import pandas as pd\n",
      "from multiprocessing import Pool\n",
      "\n",
      "# Your DataFrame\n",
      "data = {'A': [1, 2, 3, 4], 'B': [10, 20, 30, 40]}\n",
      "df = pd.DataFrame(data)\n",
      "\n",
      "# Define functions that take the dataframe and additional arguments\n",
      "def add_column1(df, factor):\n",
      "    df['C'] = (df['A'] + df['B']) * factor\n",
      "    return df\n",
      "\n",
      "def add_column2(df, addend):\n",
      "    df['D'] = df['A'] * df['B'] + addend\n",
      "    return df\n",
      "\n",
      "# List of functions and arguments to pass to each function\n",
      "tasks = [\n",
      "    (add_column1, df.copy(), 2),  # add_column1 function with factor 2\n",
      "    (add_column2, df.copy(), 5),  # add_column2 function with addend 5\n",
      "]\n",
      "\n",
      "# This function is used to unpack the arguments and invoke the function with them\n",
      "def apply_function(func, df, *args):\n",
      "    return func(df, *args)\n",
      "\n",
      "# Use multiprocessing to apply the functions to the DataFrame in parallel\n",
      "if __name__ == '__main__':  # This check is necessary for multiprocessing\n",
      "    with Pool() as pool:\n",
      "        results = pool.starmap(apply_function, tasks)  # Unpacks and applies each tuple in tasks\n",
      "\n",
      "# Combine the results back into a single DataFrame\n",
      "final_df = df.copy()\n",
      "for result in results:\n",
      "    # Merge the new columns using the column names\n",
      "    final_df = final_df.merge(result.iloc[:, len(df.columns):], left_index=True, right_index=True)\n",
      "\n",
      "# Print the final DataFrame\n",
      "print(final_df)\n",
      "```\n",
      "\n",
      "Here is what we changed:\n",
      "\n",
      "- We are using `pool.starmap()` which is designed to take an iterable of argument tuples, allowing us to pass multiple arguments to the target function. Each tuple in the iterable corresponds to the arguments for one function call.\n",
      "- Inside the `if __name__ == '__main__':` block is necessary to avoid recursion in the `__main__` module when the new processes start on Windows (not necessary on Unix-like systems but still good practice).\n",
      "- Instead of generating multiple DataFrames with different columns and then trying to concatenate them, we simply merge the new columns from the results into the original `final_df`. We make sure to only get new columns by slicing from `len(df.columns)` onwards.\n",
      "\n",
      "This approach will pass a separate copy of the DataFrame to each function along with their specific arguments to create new columns in parallel. After computations, the results are merged back into the original DataFrame.\n",
      "\n",
      "User's input was:  how to pass kwargs?\n",
      "To pass keyword arguments (kwargs) to functions running in a multiprocessing pool, you can still use the `pool.starmap()` method. `starmap()` allows you to provide an iterable of argument tuples where each tuple contains all the arguments that should be passed to the worker function for each respective call, including both positional and keyword arguments. Since the `starmap()` only works with positional arguments, you can modify the worker function to accept the kwargs as a dictionary and then use the `**` syntax to pass them to your target function.\n",
      "\n",
      "Here is an example:\n",
      "\n",
      "```python\n",
      "import pandas as pd\n",
      "from multiprocessing import Pool\n",
      "\n",
      "# Your DataFrame\n",
      "data = {'A': [1, 2, 3, 4], 'B': [10, 20, 30, 40]}\n",
      "df = pd.DataFrame(data)\n",
      "\n",
      "# Define functions that take the dataframe and any number of keyword arguments\n",
      "def add_column1(df, **kwargs):\n",
      "    factor = kwargs.get('factor', 1)\n",
      "    df['C'] = (df['A'] + df['B']) * factor\n",
      "    return df\n",
      "\n",
      "def add_column2(df, **kwargs):\n",
      "    addend = kwargs.get('addend', 0)\n",
      "    df['D'] = df['A'] * df['B'] + addend\n",
      "    return df\n",
      "\n",
      "# Worker function that unpacks the args and kwargs and calls the provided function\n",
      "def apply_function_with_kwargs(func, df, kwargs):\n",
      "    return func(df, **kwargs)\n",
      "\n",
      "# List of functions and their corresponding keyword arguments\n",
      "tasks = [\n",
      "    (add_column1, df.copy(), {'factor': 2}),  # 'factor' will be passed to add_column1 as a kwarg\n",
      "    (add_column2, df.copy(), {'addend': 5}),  # 'addend' will be passed to add_column2 as a kwarg\n",
      "]\n",
      "\n",
      "# Use multiprocessing to apply the functions to the DataFrame in parallel\n",
      "if __name__ == '__main__':  # This guard is necessary for multiprocessing\n",
      "    with Pool() as pool:\n",
      "        # Use pool.starmap to execute each function with its args and kwargs\n",
      "        results = pool.starmap(apply_function_with_kwargs, tasks)\n",
      "\n",
      "# Combine the results back into a single DataFrame\n",
      "final_df = df.copy()\n",
      "for result in results:\n",
      "    # Merge the new columns using the column names\n",
      "    final_df = final_df.merge(result.iloc[:, len(df.columns):], left_index=True, right_index=True)\n",
      "\n",
      "# Example of the final DataFrame\n",
      "print(final_df)\n",
      "```\n",
      "\n",
      "In the `apply_function_with_kwargs` function, we pass the `kwargs` dictionary to the corresponding function. The `**kwargs` syntax unpacks the dictionary into keyword arguments.\n",
      "\n",
      "Please remember that passing large DataFrames between processes can be inefficient because of the need to serialize and deserialize the DataFrames for inter-process communication. For large-scale problems, it may be more suitable to use libraries specifically designed for distributed computing, such as Dask, which handles large data more efficiently in parallel computation scenarios.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "message_history = []\n",
    "title = \"multiprocessing\"\n",
    "time = datetime.now().strftime(\"%Y-%m-%d_%H:%M:%S\")\n",
    "# model = \"gpt-3.5-turbo\"\n",
    "# \"gpt-4-1106-preview\"\n",
    "# \"gpt-3.5-turbo-1106\"\n",
    "# \"gpt-4\"\n",
    "model = \"gpt-4-1106-preview\"\n",
    "\n",
    "def chat(inp, model=model, role=\"user\"):\n",
    "    message_history.append({\"role\": role, \"content\": f\"{inp}\"})\n",
    "    completion = openai.ChatCompletion.create(\n",
    "        model=model,\n",
    "        messages=message_history\n",
    "    )\n",
    "    reply_content = completion.choices[0].message.content\n",
    "    message_history.append({\"role\": \"assistant\", \"content\": f\"{reply_content}\"})\n",
    "    return reply_content\n",
    "\n",
    "def save_chat(history, title, time):\n",
    "    with open(f\"chat/{title}_{time}.txt\", \"w\") as f:\n",
    "        f.write(f\"model: {model}\\n\")\n",
    "        for message in history:\n",
    "            f.write(f\"{message['role']}: {message['content']}\\n\")\n",
    "            \n",
    "\n",
    "while True:\n",
    "    user_input = input(\"> \")\n",
    "    if user_input == \"quit\":\n",
    "        save_chat(message_history, title, time)\n",
    "        break\n",
    "    print(\"User's input was: \", user_input)\n",
    "    print(chat(user_input))\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
